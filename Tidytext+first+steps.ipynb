{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidytext first steps\n",
    "\n",
    "This notebook was posted by Simon Lindgren // [@simonlindgren](http://www.twitter.com/simonlindgren) // [simonlindgren.com](http://simonlindgren.com)\n",
    "\n",
    "Data scientists Julia Silge and David Robinson have developed the R package [tidytext](http://joss.theoj.org/papers/10.21105/joss.00037), the aim of which is to make text mining workflows more efficient by treating text as data frames of individual words.\n",
    "\n",
    "Storing words as rows in dataframes is different to how text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an [n-gram](https://en.wikipedia.org/wiki/N-gram), sentence, or paragraph.\n",
    "\n",
    "Tidytext is written to work together with [_tidyverse_](http://tidyverse.org) â€” a collection of R packages that share common principles and are designed to work together seamlessly.\n",
    "\n",
    "When creating this notebook, I drew heavily on [the book](http://tidytextmining.com/) by the creators of the package.\n",
    "\n",
    "\n",
    "Install tidyverse by opening an R prompt in Terminal and enter `install.packages(\"tidyverse\")`, then install tidytext with `install.packages(\"tidytext\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidytext leverages the `readr` package to read documents. The function `read_csv2` reads csv files with ';' as column separator.\n",
    "\n",
    "`readr` is great in many ways. For example, it often successfully guesses what data format the columns are in. See its documentation [here](https://github.com/tidyverse/readr).\n",
    "\n",
    "We read the csv into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents <- read_csv2(\"tidyraw.csv\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidyfy the documents\n",
    "To work with our documents as a tidy dataset, let's restructure the dataframe in the one-token-per-row format using the `unnest_tokens` function in `tidytext`. The parentheses, for example `(text, text)`, states (the column to be written, the column to read).\n",
    "\n",
    "This makes use of the [`tokenizers`](https://github.com/ropensci/tokenizers) package to separate the text from the initial csv into tokens. The default is to tokenize by words. The code below also includes alternatives for ngrams and...\n",
    "\n",
    "The procedure retains any other columns, and strips any punctuation from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tidy_documents <- documents %>%\n",
    "    unnest_tokens(word,text)\n",
    "    #unnest_tokens(ngram, text, token = \"ngrams\", n = 2)\n",
    "tidy_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter rows\n",
    "Filter away some of the rows, using the `filter()` function in `dplyr`. In this case, the rows where the content of the column 'word' consists of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tidy_documents<- tidy_documents %>%\n",
    "    filter(!grepl(\"[0-9]\", word))\n",
    "tidy_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "The code below can be used to remove either the standard English stop words, or a list of custom stopwords from a file on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove built in English stop words\n",
    "data(stop_words)\n",
    "tidy_documents <- anti_join(tidy_documents, stop_words, by=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create custom stop word list\n",
    "my_stop_words <- read_csv2(\"swestop.csv\")\n",
    "#my_stop_words <- read_csv2(\"swestop_custom.csv\")\n",
    "\n",
    "my_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove custom stop words\n",
    "tidy_documents <- anti_join(tidy_documents, my_stop_words, by=\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the most common words\n",
    "As we now have our corpus stored as a tidy dataframe, we can make use of the manipulation grammar provided by the [`dplyr` package](https://github.com/tidyverse/dplyr) to select, filter, and arrange the data.\n",
    "\n",
    "There is also lots of useful stuff about what can be done with data in this format in Grolemund & Wickham's book [R for Data Science](http://r4ds.had.co.nz).\n",
    "\n",
    "Now, let's see which words are the most frequent ones in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tidy_documents %>%\n",
    "    count(word, sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also because we use tidy tools, we can pipe our data directly to the [`ggplot2` package](http://ggplot2.tidyverse.org) to visualise things. The code below is one example. Note that the `filter` sets the threshold for how many times a word must occur to be shown in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tidy_documents %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  filter(n > 2) %>%\n",
    "  mutate(word = reorder(word, n)) %>%\n",
    "  ggplot(aes(word, n)) +\n",
    "  geom_col() +\n",
    "  xlab(NULL) +\n",
    "  coord_flip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
